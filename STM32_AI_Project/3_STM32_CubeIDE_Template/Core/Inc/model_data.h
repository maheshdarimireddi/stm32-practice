/*
 * Quantized Fire Detection Model (int8)
 * Generated by: stm32_model_converter.py
 * Model Size: ~50KB
 */

#ifndef __MODEL_DATA_H__
#define __MODEL_DATA_H__

#include <stdint.h>

// Model metadata
#define MODEL_INPUT_SIZE 1024      // 32x32 RGB image
#define MODEL_OUTPUT_SIZE 2        // [no_fire, fire]
#define MODEL_QUANTIZATION_SCALE 0.0078125f  // 1/128
#define MODEL_QUANTIZATION_ZERO 0

// Quantized model weights (int8)
// In production, this is generated by stm32_model_converter.py
const uint8_t model_data[] = {
    // TensorFlow Lite model binary
    // Placeholder: real model would be ~50KB
    0x4C, 0x49, 0x54, 0x45, // "LITE" header
    0x00, 0x00, 0x00, 0x00,
    // ... actual model binary data ...
};

const uint32_t model_data_len = sizeof(model_data);

// Model information structure
typedef struct {
    const char* model_name;
    const char* model_version;
    uint32_t input_width;
    uint32_t input_height;
    uint32_t input_channels;
    float confidence_threshold;
} ModelInfo;

static const ModelInfo model_info = {
    .model_name = "FireDetectionV2",
    .model_version = "2.0",
    .input_width = 32,
    .input_height = 32,
    .input_channels = 3,
    .confidence_threshold = 0.7f
};

#endif // __MODEL_DATA_H__
